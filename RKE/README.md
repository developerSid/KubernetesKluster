# RKE (Rancher Kubernetes Engine)

## Tools Required
1. [RKE](https://github.com/rancher/rke/releases)
   1. Make sure to grab the most recent stable release for your platform, expand the archive, and put the binary on your path
   1. The assumption in this document is that the binary has been named `rke`
1. [Groovy](http://groovy-lang.org/)
   1. Groovy was used since by trade I am a Java Developer, and I prefered it's templating syntax to trying to hack together
      a bunch of Google'd bash scripts that would do awk find and replaces (Sorry)
1. [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
   1. There is no simple install instructions for this.  If you are lucky the package manager on your machine will have it available :)
   
## Vagrant (VirtualBox)

### Six Node Cluster
1. `cd RKE/vagrant/virtualbox/sixnode`
1. Execute the RKE template generation script `./rancher-cluster-config.groovy`
1. `cd out`
1. Execute the Kubernetes install `rke up`

### Three Node Cluster
1. `cd RKE/vagrant/virtualbox/twonode`
1. Execute the RKE template generation script `./rancher-cluster-config.groovy`
1. `cd out`
1. Execute the Kubernetes install `rke up`


## Kubectl
Once the install finishes for any of the clusters that was used a file called *kube_config_cluster.yml* will be generated by RKE.  
This file is the configuration for `kubectl` and can be used to manage the cluster.  There is one minor change that isn't required,
but the cluster is configured to handle.  I believe that in order for the system to be more highly available (see the Six Node Cluster),
that it is necessary to have a layer 4 load balancer in front of the 3 API Servers which is what `kubectl` communicates with.  With
that in mind the change that needs to be made to the *kube_config_cluster.yml* file for the vagrant/virtualbox managed cluster is 
changing `server: "https://192.168.50.11:6443"` to `server: "https://192.168.50.10:6443"`.

Once you have you can now issue `kubectl` commands to the cluster
1. `kubectl --kubeconfig kube_config_cluster.yml get nodes -o wide`
   1. For the three node cluster would look something like
   ```bash
   NAME            STATUS    ROLES               AGE       VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
   192.168.50.11   Ready     controlplane,etcd   14m       v1.11.1   192.168.50.11   <none>        Ubuntu 16.04.3 LTS   4.4.0-87-generic   docker://17.3.1
   192.168.50.20   Ready     worker              14m       v1.11.1   192.168.50.20   <none>        Ubuntu 16.04.3 LTS   4.4.0-87-generic   docker://17.3.1
   ```
1. `kubectl --kubeconfig kube_config_cluster.yml get pods --all-namespaces`
   1. For the three node cluster would look something like
   ```bash
   NAMESPACE       NAME                                      READY     STATUS      RESTARTS   AGE
   ingress-nginx   default-http-backend-797c5bc547-xt8dg     1/1       Running     0          15m
   ingress-nginx   nginx-ingress-controller-nqn55            1/1       Running     0          15m
   kube-system     canal-9k7v5                               3/3       Running     0          15m
   kube-system     canal-mnkn7                               3/3       Running     0          15m
   kube-system     kube-dns-68d4dd4db7-xmvs5                 3/3       Running     0          15m
   kube-system     kube-dns-autoscaler-5db9bbb766-qmbpq      1/1       Running     0          15m
   kube-system     metrics-server-97bc649d5-ns7j5            1/1       Running     0          15m
   kube-system     rke-ingress-controller-deploy-job-fnrgv   0/1       Completed   0          15m
   kube-system     rke-kubedns-addon-deploy-job-ln9r6        0/1       Completed   0          15m
   kube-system     rke-metrics-addon-deploy-job-8f7sb        0/1       Completed   0          15m
   kube-system     rke-network-plugin-deploy-job-5zrxt       0/1       Completed   0          15m
   ```

### Global configuration
The *kube_config_cluster.yml* can copied placed in *~/.kube/config*.  Make sure the *~/.kube/* directory has been created.
TODO: Need to look if there is a more elegant way of handling this via `kubectl` since this overwrites any other `kubectl` configurations that may 
      already be setup